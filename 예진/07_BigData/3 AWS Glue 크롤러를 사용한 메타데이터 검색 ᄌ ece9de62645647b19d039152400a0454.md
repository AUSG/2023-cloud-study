# 3. AWS Glue 크롤러를 사용한 메타데이터 검색 자동화

<br>
<br>

### 실습

---

- S3의 CSV 파일을 추가 분석 및 쿼리 작업에 사용하고자 파일에 대한 스키마 및 메타데이터를 확인해보자
- AWS Glue 데이터베이스를 생성 → 크롤러 구성 마법사에 따라 S3 버킷 데이터 스캔하도록 크롤러 구성 → 크롤러 실행 → 결과 테이블 확인

1. AWS Glue 콘솔 → `Data Catalog` 선택 → `Add database` 선택 → 데이터베이스 이름 지정하고 생성 클릭
2. glue 콘솔 → `tables` → **`Add tables using crawler`**
   1. 703crawler
   2. `Add a data source`
      1. data source : S3
      2. Location of S3 data : In a different account
      3. S3 path : 사전에 만든 버킷 `cookbook703bucket`
   3. IAM role 생성 : AWSGlueServiceRole-AWSCookbook703
   4. Output configuration : 만든 데이터베이스 선택 : `703database`
   5. Frequency : On demand
3. glue 콘솔 → `creawlers` → 생성한 `703crawler` 선택 → 실행
4. 생성한 크롤러 구성 확인 → LastCrawl 상태가 SUCCEEDED 로 변경된 것을 확인

   ```bash
   aws glue get-crawler --name 703crawler
   ```

5. glue 콘솔에서 생성한 테이블을 선택하고 `속성 보기`를 클릭하여 JSON 정보 확인

   ```bash
   # 코드로 확인
   aws glue get-table --database-name 703database --name data
   ```

<br>
<br>

### 정리

---

`AWS Glue` : 데이터 ETL(Extract, Transform, Load) 작업을 쉽게 수행할 수 있도록 도와주는 완전 관리형 서비스

`AWS Glue 크롤러` : Glue의 기능 중 하나로, 데이터 소스를 자동으로 검색하여 스키마 정보를 수집하는 서비스
<br>

**해당 레시피 해석**

---

크롤러를 사용해 S3로 수집 및 저장한 대량의 데이터가 어떻게 분할 됐는지, 구조화된 데이터인지, 데이터 크기, 데이터 스키마를 정의하는 쿼리 기능 및 메타 데이터 검색을 수행했다.

크롤러는 S3(=데이터 소스)에 연결하고, 소스의 객체를 스캔하고, 데이터 스키마와 연결된 테이블로 Glue 데이터 카탈로그 데이터베이스를 채웠다.
